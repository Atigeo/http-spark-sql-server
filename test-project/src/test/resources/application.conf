domain{
akka {
  //loglevel = "DEBUG"
  actor {
    provider = "akka.remote.RemoteActorRefProvider"
  }
  remote {
     enabled-transports = ["akka.remote.netty.tcp"]
    log-sent-messages = on
    log-received-messages = on
    netty.tcp {
     transport-class = "akka.remote.transport.netty.NettyTransport"
      hostname = "devbox.local"
      port = 4042
    }
  }
}
}

cancel{
akka {
  //loglevel = "DEBUG"
  actor {
    provider = "akka.remote.RemoteActorRefProvider"
  }
  remote {
     enabled-transports = ["akka.remote.netty.tcp"]
    log-sent-messages = on
    log-received-messages = on
    netty.tcp {
     transport-class = "akka.remote.transport.netty.NettyTransport"
      hostname = "devbox.local"
      port = 7071
    }
  }
}
}
############ spark configuration - see spark documentation ####################
sparkConfiguration {
	spark-executor-memory=2g
	spark-mesos-coarse=false
	spark-scheduler-mode=FAIR
	spark-cores-max=2
	spark-master="spark://10.0.2.16:7077"
	spark-path="/home/ubuntu/latest-mssh/spark-1.0.1"
	spark-default-parallelism=384
	spark-storage-memoryFraction=0.3
	spark-shuffle-memoryFraction=0.6
	spark-shuffle-compress=true
	spark-shuffle-spill-compress=true
	spark-reducer-maxMbInFlight=48
	spark-akka-frameSize=10000
	spark-akka-threads=4
	spark-akka-timeout=100
	spark-task-maxFailures=4
	spark-shuffle-consolidateFiles=true
	spark-deploy-spreadOut=true
	spark-shuffle-spill=false
}

######### application configuration ###################
appConf{
	# the cors filter allowed hosts
	cors-filter-allowed-hosts="*"
	# the default number of results retrieved on queries
	nr.of.results=100
	# the hdfs namenode - it is used when querying with unlimited number of results.
	jaws.namenode="hdfs://10.0.2.16"
	# the remote doamain actor address
	remote.domain.actor=""
	#remote.domain.actor="akka.tcp://domainSystem@devbox.local:5052/user/RunScript"
	# application name
	application.name="Jaws"
	# the port on which to deploy the apis
	web.services.port=8181
	# the port on which to deploy the web sockets api (logs)
	web.sockets.port=8182
	# the number of threads used to execute shark commands
	nr.of.threads=10
	# implicit akka timeout
	timeout=1000000
	#where to log:  app.logging.type = cassandra/hdfs
	app.logging.type=cassandra
	# folder where to write the results schema
	schemaFolder=jawsSchemaFolder
	# the path to the xpatterns-jaws in target folder
	jar-path=/home/ema/work/jawsGit/http-spark-sql-server/test-project/target/test-app.jar
}

########## hadoop configuration - skip this if you are using cassandra logging ########
hadoopConf {
	namenode="hdfs://devbox.local:8020"
	replicationFactor=1
	# set on true if you want to start fresh (all the existing folders will be recreated)
	forcedMode=false
	# folder where to write the logs
	loggingFolder=jawsLogs
	# folder where to write the jobs states
	stateFolder=jawsStates
	# folder where to write the jobs details
	detailsFolder=jawsDetails
	# folder where to write the jobs results
	resultsFolder=jawsResultsFolder
	# folder where to write the jobs meta information
	metaInfoFolder=jawsMetainfoFolder

}

########## cassandra configuration - skip this if you are using hdfs logging ##########
cassandraConf {
	cassandra.host="devbox.local:9160"
	cassandra.keyspace=xpatterns_jaws
	cassandra.cluster.name=Jaws
}
